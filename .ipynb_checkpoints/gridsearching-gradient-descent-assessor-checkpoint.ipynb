{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Gradient Descent in Sklearn\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Until now we've been using specific sklearn model classes to perform regression and classification such as `LinearRegression` and `LogisticRegression`. Unfortunately, while these methods work well on smaller datasets with relatively small numbers of columns, once you start getting into \"Medium Data\" these slow down to a crawl, and take up so much memory that fitting them becomes mind-numbingly slow (especially on a laptop).\n",
    "\n",
    "Luckily, sklearn comes with  stochastic gradient descent solvers for regression and classification:\n",
    "- `SGDRegressor`\n",
    "- `SGDClassifier`\n",
    "\n",
    "Due to its ability to minimize the loss function iteratively on smaller portions of the data, it avoids the intense slowdown other models suffer on large datasets.\n",
    "\n",
    "> **Note:** The gradient descent solvers are very flexible and can fit a variety of different model types not covered here. I highly recommend reading their documentation in detail.\n",
    "\n",
    "---\n",
    "\n",
    "### SF assessor data\n",
    "\n",
    "This lab uses data from the SF assessor's office on housing prices in San Francisco - it's already cleaned up.\n",
    "\n",
    "You can see that the dataset has 250k rows. When expanding this with dummy-coded categorical columns it can become quite large. Be careful that you don't exceed the memory on your computer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy \n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "\n",
    "import patsy\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data.\n",
    "\n",
    "Examine the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prop = pd.read_csv('../datasets/assessor_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250000, 17)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "baths               int64\n",
       "beds                int64\n",
       "lot_depth         float64\n",
       "basement_area     float64\n",
       "front_ft          float64\n",
       "owner_pct         float64\n",
       "rooms               int64\n",
       "property_class     object\n",
       "neighborhood       object\n",
       "tax_rate          float64\n",
       "volume              int64\n",
       "sqft                int64\n",
       "stories             int64\n",
       "year_recorded       int64\n",
       "year_built          int64\n",
       "zone               object\n",
       "value             float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "baths             0\n",
       "beds              0\n",
       "lot_depth         0\n",
       "basement_area     0\n",
       "front_ft          0\n",
       "owner_pct         0\n",
       "rooms             0\n",
       "property_class    0\n",
       "neighborhood      0\n",
       "tax_rate          0\n",
       "volume            0\n",
       "sqft              0\n",
       "stories           0\n",
       "year_recorded     0\n",
       "year_built        0\n",
       "zone              0\n",
       "value             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['D', 'Z', 'DBM', 'LZ', 'TH', 'ZBM'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop.property_class.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['01E', '10G', '10C', '02C', '02A', '04S', '08F', '10H', '03A',\n",
       "       '02F', '10B', '04T', '04J', '04M', '08E', '10F', '10E', '03G',\n",
       "       '03H', '06D', '09E', '02B', '02D', '10D', '01C', '07B', '02E',\n",
       "       '04C', '04H', '10A', '05A', '04F', '04E', '10J', '09A', '03B',\n",
       "       '07C', '01A', '01G', '09G', '02G', '05C', '05F', '03J', '05K',\n",
       "       '03E', '06A', '05D', '08C', '04G', '04R', '06B', '01D', '07A',\n",
       "       '09C', '09F', '10K', '01F', '05G', '03C', '09H', '04B', '08G',\n",
       "       '04N', '05B', '08A', '08H', '04P', '03F', '07D', '06E', '01B',\n",
       "       '05M', '05E', '06C', '04D', '05H', '08B', '05J', '09D', '04A',\n",
       "       '04K', '08D', '06F', '03D', '09B', '047', '08I'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop.neighborhood.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sample down the data\n",
    "\n",
    "Despite this already being a sample of the full assessor dataset, you should sample the data down further the sake of speed and your computers memory.\n",
    "\n",
    "Use the `.sample()` function for pandas dataframes to subset this down to < 25000 rows. \n",
    "\n",
    "Sampling down large datasets is a common procedure. Finding the optimal parameters with larger subsets of the data may change the hyperparameters and the results, and will get you closer to the best coefficients, but the returns are marginal at a point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prop_samp = prop.sample(n=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Regression with stochastic gradient descent\n",
    "\n",
    "Below I set up X, y data predicting value (housing price) from the remaining variables. There are ~75,000 rows, with 170 columns.\n",
    "\n",
    "\n",
    "The `SGDRegressor` is very general and flexible, and can be customized with a variety of keyword arguments.\n",
    "\n",
    "**Arguments**\n",
    "- `loss`: `['squared_loss','huber', ...]`\n",
    "    - The `'squared_loss'` loss corresponds to solving a regression with the least squares loss. This is what I expect you'll use, but there are other options. Huber loss is a \"robust\" regression loss.\n",
    "- `penalty`: `['none','l1','l2','elasticnet']`\n",
    "    - This defines the penalty on the regression that you would like to solve. The l1 and l2 are the Lasso and Ridge, while the elasticnet is the combination of them both.\n",
    "- `alpha`\n",
    "    - The regularization strength to be used with a chosen penalty. Same as in Lasso and Ridge.\n",
    "- `l1_ratio`\n",
    "    - The mix of the Lasso and Ridge penalties when elasticnet is chosen as the penalty.\n",
    "- `n_iter`\n",
    "    - The number of training \"epochs\" over the data. This is the number of passes that the gradient descent algorithm will make over the data to iteratively fit the weights (defaults to 5).\n",
    "\n",
    "`SGDRegressor` is most often used in tandem with grid searching to find the optimal parameters for certain models. \n",
    "\n",
    "**It is up to you how you want to define the model. You should:**\n",
    "\n",
    "1. Choose a target to estimate (this should be continuous).\n",
    "- Select predictors to use.\n",
    "- Standardize your predictor matrix.\n",
    "- Build a stochastic gradient descent solver to fit your model. You will likely want to do some kind of gridsearch to find the optimal parameters for your model.\n",
    "- Describe the model selected through gridsearch and compare the performance to baseline.\n",
    "- Examine and interpret the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value ~ baths + beds + lot_depth + basement_area + front_ft + owner_pct + rooms + property_class + neighborhood + tax_rate + volume + sqft + stories + year_recorded + year_built + zone -1\n",
      "(25000,) (25000, 165)\n"
     ]
    }
   ],
   "source": [
    "f = 'value ~ '+' + '.join([c for c in prop_samp.columns if not c == 'value'])+' -1'\n",
    "print f\n",
    "y, X = patsy.dmatrices(f, data=prop_samp, return_type='dataframe')\n",
    "y = y.values.ravel()\n",
    "\n",
    "print y.shape, X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I am predicting the value of the house from the other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor, SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 165)\n"
     ]
    }
   ],
   "source": [
    "# standardize the predictors\n",
    "ss = StandardScaler()\n",
    "Xs = ss.fit_transform(X)\n",
    "print Xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up my gridsearch parameters:\n",
    "sgd_params = {\n",
    "    'loss':['squared_loss','huber'],\n",
    "    'penalty':['l1','l2'],\n",
    "    'alpha':np.logspace(-5,1,25)\n",
    "}\n",
    "\n",
    "sgd_reg = SGDRegressor()\n",
    "sgd_reg_gs = GridSearchCV(sgd_reg, sgd_params, cv=5, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,\n",
       "       fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',\n",
       "       loss='squared_loss', n_iter=5, penalty='l2', power_t=0.25,\n",
       "       random_state=None, shuffle=True, verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'penalty': ['l1', 'l2'], 'loss': ['squared_loss', 'huber'], 'alpha': array([  1.00000e-05,   1.77828e-05,   3.16228e-05,   5.62341e-05,\n",
       "         1.00000e-04,   1.77828e-04,   3.16228e-04,   5.62341e-04,\n",
       "         1.00000e-03,   1.77828e-03,   3.16228e-03,   5.62341e-03,\n",
       "         1.00000e-...2341e-01,\n",
       "         1.00000e+00,   1.77828e+00,   3.16228e+00,   5.62341e+00,\n",
       "         1.00000e+01])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SGD is pretty fast compared to other sklearn solvers - but can still\n",
    "# take a good long while depending on the gridsearch and the size of\n",
    "# the dataset.\n",
    "sgd_reg_gs.fit(Xs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'penalty': 'l2', 'alpha': 0.56234132519034907, 'loss': 'squared_loss'}\n",
      "0.209142259575\n"
     ]
    }
   ],
   "source": [
    "print sgd_reg_gs.best_params_\n",
    "print sgd_reg_gs.best_score_\n",
    "sgd_reg = sgd_reg_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We can see that the gradient descent got a .22 R2 using the ridge and squared loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>mag</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>44243.825993</td>\n",
       "      <td>44243.825993</td>\n",
       "      <td>beds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>30445.480604</td>\n",
       "      <td>30445.480604</td>\n",
       "      <td>neighborhood[T.07D]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>26106.242367</td>\n",
       "      <td>26106.242367</td>\n",
       "      <td>sqft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-23550.704063</td>\n",
       "      <td>23550.704063</td>\n",
       "      <td>zone[T.C3S]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>22435.872044</td>\n",
       "      <td>22435.872044</td>\n",
       "      <td>baths</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>19841.165321</td>\n",
       "      <td>19841.165321</td>\n",
       "      <td>neighborhood[T.08B]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>18945.510497</td>\n",
       "      <td>18945.510497</td>\n",
       "      <td>neighborhood[T.05C]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>17795.888056</td>\n",
       "      <td>17795.888056</td>\n",
       "      <td>neighborhood[T.01C]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>-17221.832793</td>\n",
       "      <td>17221.832793</td>\n",
       "      <td>zone[T.RH3RM1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>17165.547402</td>\n",
       "      <td>17165.547402</td>\n",
       "      <td>neighborhood[T.07B]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             coef           mag                 pred\n",
       "153  44243.825993  44243.825993                 beds\n",
       "65   30445.480604  30445.480604  neighborhood[T.07D]\n",
       "161  26106.242367  26106.242367                 sqft\n",
       "96  -23550.704063  23550.704063          zone[T.C3S]\n",
       "152  22435.872044  22435.872044                baths\n",
       "67   19841.165321  19841.165321  neighborhood[T.08B]\n",
       "47   18945.510497  18945.510497  neighborhood[T.05C]\n",
       "7    17795.888056  17795.888056  neighborhood[T.01C]\n",
       "129 -17221.832793  17221.832793       zone[T.RH3RM1]\n",
       "63   17165.547402  17165.547402  neighborhood[T.07B]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_coefs = pd.DataFrame({'coef':sgd_reg.coef_,\n",
    "                            'mag':np.abs(sgd_reg.coef_),\n",
    "                            'pred':X.columns})\n",
    "value_coefs.sort_values('mag', ascending=False, inplace=True)\n",
    "value_coefs.iloc[0:10, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Classification with stochastic gradient descent\n",
    "\n",
    "The `SGDClassifier` is very similar to the `SGDRegressor`. The main difference is that the loss functions are changed to regression loss functions.\n",
    "\n",
    "**Arguments**\n",
    "- `loss`: `['log', ...]`\n",
    "    - The `'log'` loss corresponds to solving a logistic regression classifier. This is what I expect you'll use, but there are many other options.\n",
    "- `penalty`: `['none','l1','l2','elasticnet']`\n",
    "    - This defines the penalty on the regression that you would like to solve. The l1 and l2 are the Lasso and Ridge, while the elasticnet is the combination of them both.\n",
    "- `alpha`\n",
    "    - The regularization strength to be used with a chosen penalty. Same as in Lasso and Ridge.\n",
    "- `l1_ratio`\n",
    "    - The mix of the Lasso and Ridge penalties when elasticnet is chosen as the penalty.\n",
    "- `n_iter`\n",
    "    - The number of training \"epochs\" over the data. This is the number of passes that the gradient descent algorithm will make over the data to iteratively fit the weights (defaults to 5).\n",
    "\n",
    "Like `SGDRegressor`, `SGDClassifier` is most often used in tandem with grid searching to find the optimal parameters for certain models. \n",
    "\n",
    "**It is up to you how you want to define the model. You should:**\n",
    "\n",
    "1. Choose a target to classify (you may need to engineer one from existing variables).\n",
    "- Calculate the baseline accuracy.\n",
    "- Select predictors to use.\n",
    "- Standardize your predictor matrix.\n",
    "- Build a stochastic gradient descent solver to fit your model. You will likely want to do some kind of gridsearch to find the optimal parameters for your model.\n",
    "- Describe the model selected through gridsearch and compare the performance to baseline.\n",
    "- Examine and interpret the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'baths', u'beds', u'lot_depth', u'basement_area', u'front_ft',\n",
       "       u'owner_pct', u'rooms', u'property_class', u'neighborhood', u'tax_rate',\n",
       "       u'volume', u'sqft', u'stories', u'year_recorded', u'year_built',\n",
       "       u'zone', u'value'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop_samp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1941    968\n",
       "1925    915\n",
       "1940    858\n",
       "1926    781\n",
       "1924    750\n",
       "1927    732\n",
       "1939    643\n",
       "1923    640\n",
       "1948    566\n",
       "1947    553\n",
       "1908    535\n",
       "1928    520\n",
       "1922    475\n",
       "1950    445\n",
       "1906    436\n",
       "1938    435\n",
       "1946    418\n",
       "1907    411\n",
       "1951    411\n",
       "1931    405\n",
       "1944    359\n",
       "1936    348\n",
       "1937    346\n",
       "1949    339\n",
       "1930    338\n",
       "1929    334\n",
       "1910    328\n",
       "1912    312\n",
       "1942    298\n",
       "1916    275\n",
       "       ... \n",
       "1933     97\n",
       "1993     93\n",
       "1995     88\n",
       "1977     83\n",
       "1998     83\n",
       "1985     77\n",
       "1918     72\n",
       "1976     67\n",
       "1973     62\n",
       "1966     58\n",
       "1970     56\n",
       "1994     56\n",
       "1967     50\n",
       "1903     50\n",
       "1902     47\n",
       "1974     42\n",
       "2000     42\n",
       "1968     41\n",
       "1969     32\n",
       "1999     29\n",
       "1934     25\n",
       "1971     22\n",
       "1901     10\n",
       "2001      6\n",
       "2002      2\n",
       "2005      2\n",
       "2003      2\n",
       "2006      1\n",
       "2008      1\n",
       "2004      1\n",
       "Name: year_built, Length: 107, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prop_samp.year_built.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lets see if we can predict if a house was built past 1980\n",
    "prop_samp['built_past1980'] = prop_samp.year_built.map(lambda x: 1 if x >= 1980 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.89756\n"
     ]
    }
   ],
   "source": [
    "# make the target and calculate the baseline:\n",
    "y = prop_samp.built_past1980.values\n",
    "print 1. - np.mean(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = '''\n",
    "~ baths + beds + lot_depth + basement_area + front_ft + owner_pct +\n",
    "rooms + property_class + neighborhood + tax_rate + volume + sqft + stories +\n",
    "zone + value -1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,) (25000, 164)\n"
     ]
    }
   ],
   "source": [
    "X = patsy.dmatrix(f, data=prop_samp, return_type='dataframe')\n",
    "\n",
    "Xs = ss.fit_transform(X)\n",
    "print y.shape, Xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd_cls_params = {\n",
    "    'loss':['log'],\n",
    "    'penalty':['l1','l2'],\n",
    "    'alpha':np.logspace(-5,2,50)\n",
    "}\n",
    "\n",
    "sgd_cls = SGDClassifier()\n",
    "sgd_cls_gs = GridSearchCV(sgd_cls, sgd_cls_params, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 500 out of 500 | elapsed:  6.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,\n",
       "       penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n",
       "       verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'penalty': ['l1', 'l2'], 'loss': ['log'], 'alpha': array([  1.00000e-05,   1.38950e-05,   1.93070e-05,   2.68270e-05,\n",
       "         3.72759e-05,   5.17947e-05,   7.19686e-05,   1.00000e-04,\n",
       "         1.38950e-04,   1.93070e-04,   2.68270e-04,   3.72759e-04,\n",
       "         5.17947e-04,   7.19686e-04,...    1.93070e+01,   2.68270e+01,   3.72759e+01,   5.17947e+01,\n",
       "         7.19686e+01,   1.00000e+02])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_cls_gs.fit(Xs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'penalty': 'l1', 'alpha': 0.00019306977288832496, 'loss': 'log'}\n",
      "0.96236\n"
     ]
    }
   ],
   "source": [
    "print sgd_cls_gs.best_params_\n",
    "print sgd_cls_gs.best_score_\n",
    "sgd_cls = sgd_cls_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
